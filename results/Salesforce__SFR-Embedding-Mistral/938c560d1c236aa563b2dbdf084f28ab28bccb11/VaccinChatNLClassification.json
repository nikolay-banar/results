{
  "dataset_revision": "bd27d0058bea2ad52470d9072a3b5da6b97c1ac3",
  "task_name": "VaccinChatNLClassification",
  "mteb_version": "2.10.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.652991,
            "f1": 0.680686,
            "f1_weighted": 0.641996,
            "precision": 0.667776,
            "precision_weighted": 0.698776,
            "recall": 0.761005,
            "recall_weighted": 0.652991,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.634188,
            "f1": 0.647666,
            "f1_weighted": 0.620019,
            "precision": 0.636111,
            "precision_weighted": 0.71607,
            "recall": 0.744287,
            "recall_weighted": 0.634188,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.61453,
            "f1": 0.631018,
            "f1_weighted": 0.60054,
            "precision": 0.611242,
            "precision_weighted": 0.662038,
            "recall": 0.735427,
            "recall_weighted": 0.61453,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.650427,
            "f1": 0.672148,
            "f1_weighted": 0.641167,
            "precision": 0.657253,
            "precision_weighted": 0.713277,
            "recall": 0.764893,
            "recall_weighted": 0.650427,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.664103,
            "f1": 0.675357,
            "f1_weighted": 0.654476,
            "precision": 0.664861,
            "precision_weighted": 0.719467,
            "recall": 0.752301,
            "recall_weighted": 0.664103,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.651282,
            "f1": 0.674235,
            "f1_weighted": 0.649341,
            "precision": 0.659584,
            "precision_weighted": 0.731261,
            "recall": 0.761157,
            "recall_weighted": 0.651282,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.648718,
            "f1": 0.681415,
            "f1_weighted": 0.634346,
            "precision": 0.666773,
            "precision_weighted": 0.718722,
            "recall": 0.77374,
            "recall_weighted": 0.648718,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.642735,
            "f1": 0.661381,
            "f1_weighted": 0.639171,
            "precision": 0.651308,
            "precision_weighted": 0.719037,
            "recall": 0.756446,
            "recall_weighted": 0.642735,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.617949,
            "f1": 0.661166,
            "f1_weighted": 0.594237,
            "precision": 0.642426,
            "precision_weighted": 0.643402,
            "recall": 0.756149,
            "recall_weighted": 0.617949,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.648718,
            "f1": 0.654358,
            "f1_weighted": 0.639773,
            "precision": 0.637263,
            "precision_weighted": 0.720875,
            "recall": 0.753022,
            "recall_weighted": 0.648718,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.642564,
        "f1": 0.663943,
        "f1_weighted": 0.631507,
        "precision": 0.64946,
        "precision_weighted": 0.704293,
        "recall": 0.755843,
        "recall_weighted": 0.642564,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.663943,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 308.620968580246,
  "kg_co2_emissions": null,
  "date": 1772175401.372716
}