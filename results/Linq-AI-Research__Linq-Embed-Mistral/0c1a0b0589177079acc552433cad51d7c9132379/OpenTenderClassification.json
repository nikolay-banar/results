{
  "dataset_revision": "9af5657575a669dc18c7f897a67287ff7d1a0c65",
  "task_name": "OpenTenderClassification",
  "mteb_version": "2.10.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.612486,
            "f1": 0.608866,
            "f1_weighted": 0.608888,
            "precision": 0.614139,
            "precision_weighted": 0.614154,
            "recall": 0.612452,
            "recall_weighted": 0.612486,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.608696,
            "f1": 0.602349,
            "f1_weighted": 0.602273,
            "precision": 0.609981,
            "precision_weighted": 0.609917,
            "recall": 0.608765,
            "recall_weighted": 0.608696,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.609365,
            "f1": 0.598644,
            "f1_weighted": 0.598625,
            "precision": 0.603263,
            "precision_weighted": 0.603166,
            "recall": 0.609304,
            "recall_weighted": 0.609365,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.598662,
            "f1": 0.591741,
            "f1_weighted": 0.591697,
            "precision": 0.598725,
            "precision_weighted": 0.598702,
            "recall": 0.598731,
            "recall_weighted": 0.598662,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.601338,
            "f1": 0.597292,
            "f1_weighted": 0.597339,
            "precision": 0.601005,
            "precision_weighted": 0.601082,
            "recall": 0.601317,
            "recall_weighted": 0.601338,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.605128,
            "f1": 0.601473,
            "f1_weighted": 0.601421,
            "precision": 0.604288,
            "precision_weighted": 0.604245,
            "recall": 0.605184,
            "recall_weighted": 0.605128,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.61204,
            "f1": 0.610721,
            "f1_weighted": 0.61071,
            "precision": 0.618074,
            "precision_weighted": 0.617999,
            "recall": 0.611985,
            "recall_weighted": 0.61204,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.605351,
            "f1": 0.598998,
            "f1_weighted": 0.59897,
            "precision": 0.604799,
            "precision_weighted": 0.604715,
            "recall": 0.605322,
            "recall_weighted": 0.605351,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.586845,
            "f1": 0.572012,
            "f1_weighted": 0.571912,
            "precision": 0.57792,
            "precision_weighted": 0.577754,
            "recall": 0.586885,
            "recall_weighted": 0.586845,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.601338,
            "f1": 0.59426,
            "f1_weighted": 0.594193,
            "precision": 0.596711,
            "precision_weighted": 0.596568,
            "recall": 0.601345,
            "recall_weighted": 0.601338,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.604125,
        "f1": 0.597636,
        "f1_weighted": 0.597603,
        "precision": 0.602891,
        "precision_weighted": 0.60283,
        "recall": 0.604129,
        "recall_weighted": 0.604125,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.597636,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 220.34558296203613,
  "kg_co2_emissions": null,
  "date": 1772127279.27927
}