{
  "dataset_revision": "9af5657575a669dc18c7f897a67287ff7d1a0c65",
  "task_name": "OpenTenderClassification",
  "mteb_version": "2.9.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.43835,
            "f1": 0.432186,
            "f1_weighted": 0.432255,
            "precision": 0.47077,
            "precision_weighted": 0.470921,
            "recall": 0.438334,
            "recall_weighted": 0.43835,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.382386,
            "f1": 0.374491,
            "f1_weighted": 0.374301,
            "precision": 0.403865,
            "precision_weighted": 0.403765,
            "recall": 0.382611,
            "recall_weighted": 0.382386,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.418729,
            "f1": 0.413002,
            "f1_weighted": 0.41294,
            "precision": 0.438517,
            "precision_weighted": 0.438367,
            "recall": 0.418757,
            "recall_weighted": 0.418729,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.423188,
            "f1": 0.409831,
            "f1_weighted": 0.409754,
            "precision": 0.440516,
            "precision_weighted": 0.440492,
            "recall": 0.423265,
            "recall_weighted": 0.423188,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.409142,
            "f1": 0.39813,
            "f1_weighted": 0.398159,
            "precision": 0.413265,
            "precision_weighted": 0.413338,
            "recall": 0.409156,
            "recall_weighted": 0.409142,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.402899,
            "f1": 0.394545,
            "f1_weighted": 0.39443,
            "precision": 0.435198,
            "precision_weighted": 0.435192,
            "recall": 0.403063,
            "recall_weighted": 0.402899,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.400223,
            "f1": 0.391161,
            "f1_weighted": 0.391293,
            "precision": 0.416337,
            "precision_weighted": 0.416502,
            "recall": 0.400146,
            "recall_weighted": 0.400223,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.416722,
            "f1": 0.413943,
            "f1_weighted": 0.413956,
            "precision": 0.457765,
            "precision_weighted": 0.457857,
            "recall": 0.416707,
            "recall_weighted": 0.416722,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.390635,
            "f1": 0.369191,
            "f1_weighted": 0.369021,
            "precision": 0.407215,
            "precision_weighted": 0.407128,
            "recall": 0.390797,
            "recall_weighted": 0.390635,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.379264,
            "f1": 0.366799,
            "f1_weighted": 0.366669,
            "precision": 0.405603,
            "precision_weighted": 0.405423,
            "recall": 0.379344,
            "recall_weighted": 0.379264,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.406154,
        "f1": 0.396328,
        "f1_weighted": 0.396278,
        "precision": 0.428905,
        "precision_weighted": 0.428899,
        "recall": 0.406218,
        "recall_weighted": 0.406154,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.396328,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 216.50744104385376,
  "kg_co2_emissions": null,
  "date": 1772108638.45025
}