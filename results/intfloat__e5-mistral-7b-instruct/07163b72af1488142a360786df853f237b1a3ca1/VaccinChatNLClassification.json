{
  "dataset_revision": "bd27d0058bea2ad52470d9072a3b5da6b97c1ac3",
  "task_name": "VaccinChatNLClassification",
  "mteb_version": "2.9.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.37265,
            "f1": 0.456207,
            "f1_weighted": 0.358433,
            "precision": 0.485896,
            "precision_weighted": 0.572608,
            "recall": 0.58248,
            "recall_weighted": 0.37265,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.361538,
            "f1": 0.475429,
            "f1_weighted": 0.354904,
            "precision": 0.51868,
            "precision_weighted": 0.573948,
            "recall": 0.569386,
            "recall_weighted": 0.361538,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.38547,
            "f1": 0.471571,
            "f1_weighted": 0.381567,
            "precision": 0.526546,
            "precision_weighted": 0.574866,
            "recall": 0.569913,
            "recall_weighted": 0.38547,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.376068,
            "f1": 0.466804,
            "f1_weighted": 0.368078,
            "precision": 0.520605,
            "precision_weighted": 0.57123,
            "recall": 0.568455,
            "recall_weighted": 0.376068,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.371795,
            "f1": 0.442827,
            "f1_weighted": 0.363812,
            "precision": 0.486923,
            "precision_weighted": 0.533987,
            "recall": 0.561582,
            "recall_weighted": 0.371795,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.380342,
            "f1": 0.469868,
            "f1_weighted": 0.368137,
            "precision": 0.510865,
            "precision_weighted": 0.599902,
            "recall": 0.576226,
            "recall_weighted": 0.380342,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.376068,
            "f1": 0.4731,
            "f1_weighted": 0.364384,
            "precision": 0.508903,
            "precision_weighted": 0.621432,
            "recall": 0.567248,
            "recall_weighted": 0.376068,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.38547,
            "f1": 0.452492,
            "f1_weighted": 0.370875,
            "precision": 0.475574,
            "precision_weighted": 0.526682,
            "recall": 0.575061,
            "recall_weighted": 0.38547,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.37265,
            "f1": 0.477083,
            "f1_weighted": 0.36649,
            "precision": 0.51627,
            "precision_weighted": 0.536551,
            "recall": 0.577583,
            "recall_weighted": 0.37265,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.371795,
            "f1": 0.446276,
            "f1_weighted": 0.364939,
            "precision": 0.478046,
            "precision_weighted": 0.534127,
            "recall": 0.555859,
            "recall_weighted": 0.371795,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.375385,
        "f1": 0.463166,
        "f1_weighted": 0.366162,
        "precision": 0.502831,
        "precision_weighted": 0.564533,
        "recall": 0.570379,
        "recall_weighted": 0.375385,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.463166,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 268.2626314163208,
  "kg_co2_emissions": null,
  "date": 1772113825.319241
}