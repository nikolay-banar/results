{
  "dataset_revision": "bd27d0058bea2ad52470d9072a3b5da6b97c1ac3",
  "task_name": "VaccinChatNLClassification",
  "mteb_version": "2.9.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.497436,
            "f1": 0.515568,
            "f1_weighted": 0.475245,
            "precision": 0.519556,
            "precision_weighted": 0.629644,
            "recall": 0.672217,
            "recall_weighted": 0.497436,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.487179,
            "f1": 0.521315,
            "f1_weighted": 0.450235,
            "precision": 0.520858,
            "precision_weighted": 0.589374,
            "recall": 0.665716,
            "recall_weighted": 0.487179,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.515385,
            "f1": 0.548874,
            "f1_weighted": 0.487033,
            "precision": 0.554421,
            "precision_weighted": 0.599913,
            "recall": 0.685262,
            "recall_weighted": 0.515385,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.505128,
            "f1": 0.531236,
            "f1_weighted": 0.472725,
            "precision": 0.541815,
            "precision_weighted": 0.662292,
            "recall": 0.678798,
            "recall_weighted": 0.505128,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.509402,
            "f1": 0.531635,
            "f1_weighted": 0.492282,
            "precision": 0.546626,
            "precision_weighted": 0.668606,
            "recall": 0.667949,
            "recall_weighted": 0.509402,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.508547,
            "f1": 0.532153,
            "f1_weighted": 0.48951,
            "precision": 0.539166,
            "precision_weighted": 0.675178,
            "recall": 0.667173,
            "recall_weighted": 0.508547,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.512821,
            "f1": 0.529585,
            "f1_weighted": 0.485352,
            "precision": 0.525481,
            "precision_weighted": 0.589433,
            "recall": 0.664258,
            "recall_weighted": 0.512821,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.476923,
            "f1": 0.508628,
            "f1_weighted": 0.434488,
            "precision": 0.525489,
            "precision_weighted": 0.597049,
            "recall": 0.653768,
            "recall_weighted": 0.476923,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.510256,
            "f1": 0.532028,
            "f1_weighted": 0.483688,
            "precision": 0.537715,
            "precision_weighted": 0.622039,
            "recall": 0.663051,
            "recall_weighted": 0.510256,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.520513,
            "f1": 0.534242,
            "f1_weighted": 0.492855,
            "precision": 0.536993,
            "precision_weighted": 0.674441,
            "recall": 0.680493,
            "recall_weighted": 0.520513,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.504359,
        "f1": 0.528526,
        "f1_weighted": 0.476341,
        "precision": 0.534812,
        "precision_weighted": 0.630797,
        "recall": 0.669868,
        "recall_weighted": 0.504359,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.528526,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 350.1685998439789,
  "kg_co2_emissions": null,
  "date": 1772043070.856818
}